{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#import tushare as ts\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from torch.utils.data import TensorDataset\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import scipy.io as scio\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_noisy = r'Input_Patches_2Dsyn1.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(path_noisy, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2294</th>\n",
       "      <th>2295</th>\n",
       "      <th>2296</th>\n",
       "      <th>2297</th>\n",
       "      <th>2298</th>\n",
       "      <th>2299</th>\n",
       "      <th>2300</th>\n",
       "      <th>2301</th>\n",
       "      <th>2302</th>\n",
       "      <th>2303</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.214242</td>\n",
       "      <td>-0.071069</td>\n",
       "      <td>0.044274</td>\n",
       "      <td>0.061571</td>\n",
       "      <td>-0.127298</td>\n",
       "      <td>-0.106092</td>\n",
       "      <td>-0.048952</td>\n",
       "      <td>-0.076331</td>\n",
       "      <td>-0.229400</td>\n",
       "      <td>0.106868</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008154</td>\n",
       "      <td>-0.371059</td>\n",
       "      <td>0.244589</td>\n",
       "      <td>0.017343</td>\n",
       "      <td>0.040964</td>\n",
       "      <td>0.008304</td>\n",
       "      <td>0.246045</td>\n",
       "      <td>-0.152729</td>\n",
       "      <td>-0.247024</td>\n",
       "      <td>0.012803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.025840</td>\n",
       "      <td>0.018123</td>\n",
       "      <td>-0.262684</td>\n",
       "      <td>0.064054</td>\n",
       "      <td>0.050128</td>\n",
       "      <td>0.077492</td>\n",
       "      <td>0.137940</td>\n",
       "      <td>0.147687</td>\n",
       "      <td>0.300642</td>\n",
       "      <td>-0.073297</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.062408</td>\n",
       "      <td>-0.159959</td>\n",
       "      <td>0.298935</td>\n",
       "      <td>0.149444</td>\n",
       "      <td>0.144854</td>\n",
       "      <td>-0.137243</td>\n",
       "      <td>-0.113464</td>\n",
       "      <td>0.040892</td>\n",
       "      <td>0.048967</td>\n",
       "      <td>0.108723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.059878</td>\n",
       "      <td>-0.097412</td>\n",
       "      <td>-0.117858</td>\n",
       "      <td>-0.008519</td>\n",
       "      <td>0.032316</td>\n",
       "      <td>-0.115909</td>\n",
       "      <td>-0.019436</td>\n",
       "      <td>0.456654</td>\n",
       "      <td>0.167180</td>\n",
       "      <td>-0.161088</td>\n",
       "      <td>...</td>\n",
       "      <td>0.362173</td>\n",
       "      <td>0.267039</td>\n",
       "      <td>-0.167836</td>\n",
       "      <td>-0.087735</td>\n",
       "      <td>0.049714</td>\n",
       "      <td>-0.000850</td>\n",
       "      <td>-0.039238</td>\n",
       "      <td>-0.046553</td>\n",
       "      <td>-0.056353</td>\n",
       "      <td>0.202016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.179650</td>\n",
       "      <td>-0.186147</td>\n",
       "      <td>0.062173</td>\n",
       "      <td>-0.076138</td>\n",
       "      <td>0.233298</td>\n",
       "      <td>-0.003869</td>\n",
       "      <td>0.068773</td>\n",
       "      <td>0.083363</td>\n",
       "      <td>-0.038388</td>\n",
       "      <td>-0.153099</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.236150</td>\n",
       "      <td>-0.181563</td>\n",
       "      <td>-0.053868</td>\n",
       "      <td>-0.042065</td>\n",
       "      <td>0.101236</td>\n",
       "      <td>0.292420</td>\n",
       "      <td>0.012626</td>\n",
       "      <td>-0.174028</td>\n",
       "      <td>0.120366</td>\n",
       "      <td>0.081407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.431270</td>\n",
       "      <td>0.056588</td>\n",
       "      <td>0.009886</td>\n",
       "      <td>0.325598</td>\n",
       "      <td>-0.026840</td>\n",
       "      <td>0.710478</td>\n",
       "      <td>0.122607</td>\n",
       "      <td>0.057843</td>\n",
       "      <td>0.221965</td>\n",
       "      <td>-0.113067</td>\n",
       "      <td>...</td>\n",
       "      <td>0.212242</td>\n",
       "      <td>0.234083</td>\n",
       "      <td>0.089941</td>\n",
       "      <td>-0.224374</td>\n",
       "      <td>0.045691</td>\n",
       "      <td>0.119132</td>\n",
       "      <td>-0.075106</td>\n",
       "      <td>0.011380</td>\n",
       "      <td>0.044389</td>\n",
       "      <td>0.265852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33137</th>\n",
       "      <td>0.012570</td>\n",
       "      <td>0.071684</td>\n",
       "      <td>0.134780</td>\n",
       "      <td>0.058359</td>\n",
       "      <td>0.080002</td>\n",
       "      <td>-0.336990</td>\n",
       "      <td>0.035481</td>\n",
       "      <td>-0.322269</td>\n",
       "      <td>0.095744</td>\n",
       "      <td>0.037659</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.030904</td>\n",
       "      <td>-0.124197</td>\n",
       "      <td>-0.137501</td>\n",
       "      <td>-0.062988</td>\n",
       "      <td>0.163377</td>\n",
       "      <td>-0.122231</td>\n",
       "      <td>-0.162339</td>\n",
       "      <td>0.396063</td>\n",
       "      <td>0.113521</td>\n",
       "      <td>-0.261529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33138</th>\n",
       "      <td>-0.173373</td>\n",
       "      <td>-0.129791</td>\n",
       "      <td>-0.087971</td>\n",
       "      <td>0.112266</td>\n",
       "      <td>-0.073055</td>\n",
       "      <td>0.256870</td>\n",
       "      <td>-0.008347</td>\n",
       "      <td>-0.230685</td>\n",
       "      <td>-0.110658</td>\n",
       "      <td>0.011698</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.197360</td>\n",
       "      <td>0.119050</td>\n",
       "      <td>0.244596</td>\n",
       "      <td>-0.170639</td>\n",
       "      <td>0.187981</td>\n",
       "      <td>0.265022</td>\n",
       "      <td>0.136199</td>\n",
       "      <td>0.014366</td>\n",
       "      <td>-0.087579</td>\n",
       "      <td>-0.005440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33139</th>\n",
       "      <td>-0.058899</td>\n",
       "      <td>0.015288</td>\n",
       "      <td>-0.055067</td>\n",
       "      <td>0.032984</td>\n",
       "      <td>-0.006103</td>\n",
       "      <td>0.076559</td>\n",
       "      <td>-0.244653</td>\n",
       "      <td>-0.257687</td>\n",
       "      <td>-0.163633</td>\n",
       "      <td>0.102046</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.109157</td>\n",
       "      <td>0.360627</td>\n",
       "      <td>0.068984</td>\n",
       "      <td>-0.048404</td>\n",
       "      <td>-0.171367</td>\n",
       "      <td>0.011185</td>\n",
       "      <td>-0.062593</td>\n",
       "      <td>0.109995</td>\n",
       "      <td>0.107560</td>\n",
       "      <td>0.125816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33140</th>\n",
       "      <td>-0.173949</td>\n",
       "      <td>0.045912</td>\n",
       "      <td>-0.126463</td>\n",
       "      <td>-0.070584</td>\n",
       "      <td>-0.061190</td>\n",
       "      <td>-0.085109</td>\n",
       "      <td>0.047336</td>\n",
       "      <td>-0.020572</td>\n",
       "      <td>-0.073108</td>\n",
       "      <td>-0.013986</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.266267</td>\n",
       "      <td>-0.178294</td>\n",
       "      <td>-0.159218</td>\n",
       "      <td>0.012947</td>\n",
       "      <td>-0.245111</td>\n",
       "      <td>0.031110</td>\n",
       "      <td>-0.026095</td>\n",
       "      <td>-0.053649</td>\n",
       "      <td>-0.197968</td>\n",
       "      <td>0.226570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33141</th>\n",
       "      <td>-0.052753</td>\n",
       "      <td>-0.070756</td>\n",
       "      <td>-0.093865</td>\n",
       "      <td>0.055313</td>\n",
       "      <td>-0.036729</td>\n",
       "      <td>0.285653</td>\n",
       "      <td>0.031339</td>\n",
       "      <td>0.085439</td>\n",
       "      <td>0.149058</td>\n",
       "      <td>0.138616</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071480</td>\n",
       "      <td>-0.292215</td>\n",
       "      <td>-0.313345</td>\n",
       "      <td>-0.114333</td>\n",
       "      <td>-0.258362</td>\n",
       "      <td>-0.292876</td>\n",
       "      <td>0.056261</td>\n",
       "      <td>-0.152538</td>\n",
       "      <td>0.077667</td>\n",
       "      <td>-0.019683</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33142 rows × 2304 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6     \\\n",
       "0     -0.214242 -0.071069  0.044274  0.061571 -0.127298 -0.106092 -0.048952   \n",
       "1     -0.025840  0.018123 -0.262684  0.064054  0.050128  0.077492  0.137940   \n",
       "2     -0.059878 -0.097412 -0.117858 -0.008519  0.032316 -0.115909 -0.019436   \n",
       "3      0.179650 -0.186147  0.062173 -0.076138  0.233298 -0.003869  0.068773   \n",
       "4      0.431270  0.056588  0.009886  0.325598 -0.026840  0.710478  0.122607   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "33137  0.012570  0.071684  0.134780  0.058359  0.080002 -0.336990  0.035481   \n",
       "33138 -0.173373 -0.129791 -0.087971  0.112266 -0.073055  0.256870 -0.008347   \n",
       "33139 -0.058899  0.015288 -0.055067  0.032984 -0.006103  0.076559 -0.244653   \n",
       "33140 -0.173949  0.045912 -0.126463 -0.070584 -0.061190 -0.085109  0.047336   \n",
       "33141 -0.052753 -0.070756 -0.093865  0.055313 -0.036729  0.285653  0.031339   \n",
       "\n",
       "           7         8         9     ...      2294      2295      2296  \\\n",
       "0     -0.076331 -0.229400  0.106868  ...  0.008154 -0.371059  0.244589   \n",
       "1      0.147687  0.300642 -0.073297  ... -0.062408 -0.159959  0.298935   \n",
       "2      0.456654  0.167180 -0.161088  ...  0.362173  0.267039 -0.167836   \n",
       "3      0.083363 -0.038388 -0.153099  ... -0.236150 -0.181563 -0.053868   \n",
       "4      0.057843  0.221965 -0.113067  ...  0.212242  0.234083  0.089941   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "33137 -0.322269  0.095744  0.037659  ... -0.030904 -0.124197 -0.137501   \n",
       "33138 -0.230685 -0.110658  0.011698  ... -0.197360  0.119050  0.244596   \n",
       "33139 -0.257687 -0.163633  0.102046  ... -0.109157  0.360627  0.068984   \n",
       "33140 -0.020572 -0.073108 -0.013986  ... -0.266267 -0.178294 -0.159218   \n",
       "33141  0.085439  0.149058  0.138616  ...  0.071480 -0.292215 -0.313345   \n",
       "\n",
       "           2297      2298      2299      2300      2301      2302      2303  \n",
       "0      0.017343  0.040964  0.008304  0.246045 -0.152729 -0.247024  0.012803  \n",
       "1      0.149444  0.144854 -0.137243 -0.113464  0.040892  0.048967  0.108723  \n",
       "2     -0.087735  0.049714 -0.000850 -0.039238 -0.046553 -0.056353  0.202016  \n",
       "3     -0.042065  0.101236  0.292420  0.012626 -0.174028  0.120366  0.081407  \n",
       "4     -0.224374  0.045691  0.119132 -0.075106  0.011380  0.044389  0.265852  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "33137 -0.062988  0.163377 -0.122231 -0.162339  0.396063  0.113521 -0.261529  \n",
       "33138 -0.170639  0.187981  0.265022  0.136199  0.014366 -0.087579 -0.005440  \n",
       "33139 -0.048404 -0.171367  0.011185 -0.062593  0.109995  0.107560  0.125816  \n",
       "33140  0.012947 -0.245111  0.031110 -0.026095 -0.053649 -0.197968  0.226570  \n",
       "33141 -0.114333 -0.258362 -0.292876  0.056261 -0.152538  0.077667 -0.019683  \n",
       "\n",
       "[33142 rows x 2304 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.astype(np.float32)\n",
    "data= torch.from_numpy(data.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([33142, 2304])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([26513, 2304])\n",
      "torch.Size([6629, 2304])\n"
     ]
    }
   ],
   "source": [
    "#将前80%作为训练集，后20%作为测试集\n",
    "train_size = int(len(data) * 0.8)\n",
    "train = data[:train_size]\n",
    "vaild = data[train_size:]\n",
    "print(train.shape)\n",
    "print(vaild.shape)\n",
    "batch_size1 = 64\n",
    "w1 = 48\n",
    "w2 = 48\n",
    "\n",
    "train_data= TensorDataset(train)\n",
    "vaild_data= TensorDataset(vaild)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data,\n",
    "                                           batch_size = 64,\n",
    "                                           shuffle = True)\n",
    "\n",
    "vaild_loader = torch.utils.data.DataLoader(vaild_data,\n",
    "                                          batch_size = 64,\n",
    "                                          shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义Fully connected (FC) block\n",
    "class FCB(nn.Module):\n",
    "    def __init__(self, input_size, output_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "        self.activation = nn.LeakyReLU()\n",
    "        self.bn = nn.BatchNorm1d(output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = self.activation(x) \n",
    "        x = self.bn(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PAB(nn.Module):  ##position attention block\n",
    "    def __init__(self, input_size, output_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fcb1 = FCB(input_size, output_size, dropout)\n",
    "        self.fcb2 = FCB(input_size, output_size, dropout)\n",
    "        self.fcb3 = FCB(input_size, output_size, dropout)\n",
    "        self.fcb4 = FCB(input_size, output_size, dropout)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.fcb1(x)\n",
    "        x2 = self.fcb2(x)\n",
    "        x3 = self.fcb3(x)\n",
    "        x4 = self.fcb4(x)\n",
    "        \n",
    "\n",
    "        x = x1*x2\n",
    "        x = self.softmax(x)\n",
    "        x = x*x3\n",
    "        x = x+x4\n",
    "        \n",
    "        return x   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cov1d = nn.Conv1d(in_channels=1, out_channels= 64 ,kernel_size = 5, padding=0, stride=1)\n",
    "\n",
    "        self.pab1 = PAB(int(input_size-5+1), 128, dropout)\n",
    "        self.fcb1 = FCB(128, 128, dropout)\n",
    "\n",
    "        self.pab2 = PAB(128, 64, dropout)\n",
    "        self.fcb2 = FCB(64, 64, dropout)\n",
    "\n",
    "        self.pab3 = PAB(64, 32, dropout)\n",
    "        self.fcb3 = FCB(32, 32, dropout)\n",
    "\n",
    "        self.pab4 = PAB(32, 16, dropout)\n",
    "        self.fcb4 = FCB(16, 16, dropout)\n",
    "\n",
    "        self.pab5 = PAB(16, 8, dropout)\n",
    "        self.fcb5 = FCB(8, 8, dropout)\n",
    "\n",
    "        self.pab6 = PAB(8, 4, dropout)\n",
    "        self.fcb6 = FCB(4, 4, dropout)\n",
    "        \n",
    "        self.fcb7 = FCB(4, 4, dropout)\n",
    "        self.pab7 = PAB(4, 4, dropout)\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x.reshape(x.shape[0],1,x.shape[1])\n",
    "        x = self.cov1d(x)\n",
    "        x = x.permute(0,2,1)\n",
    "        x = nn.AdaptiveAvgPool1d(1)(x)\n",
    "        x = x.squeeze()\n",
    "\n",
    "        x1 = self.pab1(x)\n",
    "        x13 = self.fcb1(x1)\n",
    "\n",
    "        x2 = self.pab2(x1)\n",
    "        x12 = self.fcb2(x2)\n",
    "\n",
    "        x3 = self.pab3(x2)\n",
    "        x11 = self.fcb3(x3)\n",
    "\n",
    "        x4 = self.pab4(x3)\n",
    "        x10 = self.fcb4(x4)\n",
    "\n",
    "        x5 = self.pab5(x4)\n",
    "        x9 = self.fcb5(x5)\n",
    "\n",
    "        x6 = self.pab6(x5)\n",
    "        x8 = self.fcb6(x6)\n",
    "\n",
    "        x7 = self.fcb7(x6)\n",
    "        x7 = self.pab7(x7)\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        return x7,x8,x9,x10,x11,x12,x13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomActivation(torch.autograd.Function):    \n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        # 计算正向传播结果\n",
    "        ctx.save_for_backward(x)\n",
    "        y = 11/3 * (x - torch.tanh(x)) * torch.cos(x/2)\n",
    "        return y\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # 计算反向传播结果\n",
    "        x, = ctx.saved_tensors\n",
    "        grad_x = grad_output * (11/3 * ((torch.cos(x/2)*(1 - torch.pow(torch.cosh(x), -2)))+ (x - torch.tanh(x))*(-0.5*torch.sin(x/2))))\n",
    "                                       \n",
    "        return grad_x\n",
    "    \n",
    "class CustomActivationModule(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        return CustomActivation.apply(x)  \n",
    "    \n",
    "\n",
    "activation = CustomActivationModule()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pab1 = PAB(8, 8, dropout)\n",
    "        self.pab2 = PAB(16, 16, dropout)\n",
    "        self.pab3 = PAB(32, 32, dropout)\n",
    "        self.pab4 = PAB(64, 64, dropout)\n",
    "        self.pab5 = PAB(128, 128, dropout)\n",
    "        #self.pab5 = PAB(256, 256, dropout)\n",
    "\n",
    "        self.fc = nn.Linear(256,output_size)\n",
    "        #self.activation = nn.Identity()\n",
    "        self.activation = activation\n",
    "\n",
    "\n",
    "    def forward(self, x7,x8,x9,x10,x11,x12,x13):\n",
    "        \n",
    "        x = torch.cat((x7,x8),dim=1)\n",
    "        x = self.pab1(x)\n",
    "        x = torch.cat((x,x9),dim=1)\n",
    "        x = self.pab2(x)\n",
    "        x = torch.cat((x,x10),dim=1)\n",
    "        x = self.pab3(x)\n",
    "        x = torch.cat((x,x11),dim=1)\n",
    "        x = self.pab4(x)\n",
    "        x = torch.cat((x,x12),dim=1)\n",
    "        x = self.pab5(x)\n",
    "        x = torch.cat((x,x13),dim=1)\n",
    "        x = self.fc(x)\n",
    "        #x.requires_grad_(True)\n",
    "        x = self.activation(x)\n",
    "        #x.sum().backward()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self,input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(input_size)\n",
    "        self.decoder = Decoder(output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x7,x8,x9,x10,x11,x12,x13= self.encoder(x)\n",
    "        x = self.decoder(x7,x8,x9,x10,x11,x12,x13)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(nn.Module):\n",
    "    def __init__(self,delta,r):\n",
    "        super().__init__()\n",
    "        self.delta = delta\n",
    "        self.r = r\n",
    "    def forward(self,input,output):\n",
    "        N = input-output\n",
    "        temp = output*N\n",
    "        temp1 = data.shape[1]*(temp.sum(1))\n",
    "        temp2 = (output.sum(1))*(N.sum(1))\n",
    "        temp3 = torch.pow(data.shape[1]*((output**2).sum(1)) - ((output.sum(1))**2),1/2)\n",
    "        temp4 = torch.pow(data.shape[1]*((N**2).sum(1))-((N.sum(1))**2),1/2) \n",
    "        loss = (temp1-temp2)/(temp3*temp4)\n",
    "\n",
    "        a = torch.min(loss)\n",
    "        b = torch.mean(1 - torch.exp(-0.5 * torch.pow((N / self.delta),2)))\n",
    "        \n",
    "        return self.r*a+(1-self.r)*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss0(nn.Module):\n",
    "    def __init__(self,delta,r):\n",
    "        super().__init__()\n",
    "        self.delta = delta\n",
    "        self.r = r\n",
    "    def forward(self,input,output):\n",
    "        N = input-output\n",
    "        b = torch.mean(1 - torch.exp(-0.5 * torch.pow((N / self.delta),2)))\n",
    "        K = output.shape[0]\n",
    "        y2_mean = torch.mean(output)\n",
    "        n2_mean = torch.mean(N)\n",
    "        covariance = 0.0\n",
    "        variance_y2 = 0.0\n",
    "        variance_n2 = 0.0\n",
    "        for j in range(K):\n",
    "            y2_sample = output[j, :]\n",
    "            n2_sample = N[j, :]\n",
    "            covariance += ((y2_sample - y2_mean) * (n2_sample - n2_mean))\n",
    "            variance_y2 += ((y2_sample - y2_mean) ** 2)\n",
    "            variance_n2 += ((n2_sample - n2_mean) ** 2)\n",
    "            lcc_value = (covariance / torch.sqrt(variance_y2 * variance_n2)) ** 2\n",
    "            lcc_value = torch.mean(lcc_value)\n",
    "        return self.r*lcc_value+(1-self.r)*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") \n",
    "model = AutoEncoder(w1*w2,w1*w2).to(device)\n",
    "#将模型转移到GPU上\n",
    "#criterion = MeanHuberLoss(delta=1.3)\n",
    "#criterion = WelschLoss(delta=0.5)\n",
    "#criterion = Loss0(delta=0.46,r=0.05)#0.5 and 0.2,SNR:-8dB ok\n",
    "criterion = Loss0(delta=0.46,r=0.01)\n",
    "#criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Train Loss: 0.0992, Valid Loss: 0.0965\n",
      "loss_train:  [0.09920534301235015]\n",
      "vaild_train:  [0.09653103179656543]\n",
      "Epoch [2/100], Train Loss: 0.0891, Valid Loss: 0.0914\n",
      "loss_train:  [0.09920534301235015, 0.08908541243837541]\n",
      "vaild_train:  [0.09653103179656543, 0.09138313358506331]\n",
      "Epoch [3/100], Train Loss: 0.0826, Valid Loss: 0.0857\n",
      "loss_train:  [0.09920534301235015, 0.08908541243837541, 0.08258074100118086]\n",
      "vaild_train:  [0.09653103179656543, 0.09138313358506331, 0.08571263617621018]\n",
      "Epoch [4/100], Train Loss: 0.0795, Valid Loss: 0.0833\n",
      "loss_train:  [0.09920534301235015, 0.08908541243837541, 0.08258074100118086, 0.07952102110328445]\n",
      "vaild_train:  [0.09653103179656543, 0.09138313358506331, 0.08571263617621018, 0.08330543138659917]\n",
      "Epoch [5/100], Train Loss: 0.0773, Valid Loss: 0.0816\n",
      "loss_train:  [0.09920534301235015, 0.08908541243837541, 0.08258074100118086, 0.07952102110328445, 0.07728398374046187]\n",
      "vaild_train:  [0.09653103179656543, 0.09138313358506331, 0.08571263617621018, 0.08330543138659917, 0.08159640374091956]\n",
      "Epoch [6/100], Train Loss: 0.0764, Valid Loss: 0.0804\n",
      "loss_train:  [0.09920534301235015, 0.08908541243837541, 0.08258074100118086, 0.07952102110328445, 0.07728398374046187, 0.07640040802309311]\n",
      "vaild_train:  [0.09653103179656543, 0.09138313358506331, 0.08571263617621018, 0.08330543138659917, 0.08159640374091956, 0.08040580325401746]\n",
      "Epoch [7/100], Train Loss: 0.0760, Valid Loss: 0.0798\n",
      "loss_train:  [0.09920534301235015, 0.08908541243837541, 0.08258074100118086, 0.07952102110328445, 0.07728398374046187, 0.07640040802309311, 0.07604687459856631]\n",
      "vaild_train:  [0.09653103179656543, 0.09138313358506331, 0.08571263617621018, 0.08330543138659917, 0.08159640374091956, 0.08040580325401746, 0.0797753009515313]\n",
      "Epoch [8/100], Train Loss: 0.0759, Valid Loss: 0.0796\n",
      "loss_train:  [0.09920534301235015, 0.08908541243837541, 0.08258074100118086, 0.07952102110328445, 0.07728398374046187, 0.07640040802309311, 0.07604687459856631, 0.07594730304666313]\n",
      "vaild_train:  [0.09653103179656543, 0.09138313358506331, 0.08571263617621018, 0.08330543138659917, 0.08159640374091956, 0.08040580325401746, 0.0797753009515313, 0.07962888278640233]\n",
      "Epoch [9/100], Train Loss: 0.0753, Valid Loss: 0.0782\n",
      "loss_train:  [0.09920534301235015, 0.08908541243837541, 0.08258074100118086, 0.07952102110328445, 0.07728398374046187, 0.07640040802309311, 0.07604687459856631, 0.07594730304666313, 0.07526631778981312]\n",
      "vaild_train:  [0.09653103179656543, 0.09138313358506331, 0.08571263617621018, 0.08330543138659917, 0.08159640374091956, 0.08040580325401746, 0.0797753009515313, 0.07962888278640233, 0.07816055717949684]\n",
      "Epoch [10/100], Train Loss: 0.0749, Valid Loss: 0.0786\n",
      "loss_train:  [0.09920534301235015, 0.08908541243837541, 0.08258074100118086, 0.07952102110328445, 0.07728398374046187, 0.07640040802309311, 0.07604687459856631, 0.07594730304666313, 0.07526631778981312, 0.0749419815748571]\n",
      "vaild_train:  [0.09653103179656543, 0.09138313358506331, 0.08571263617621018, 0.08330543138659917, 0.08159640374091956, 0.08040580325401746, 0.0797753009515313, 0.07962888278640233, 0.07816055717949684, 0.07856772231081358]\n",
      "Epoch [11/100], Train Loss: 0.0749, Valid Loss: 0.0780\n",
      "loss_train:  [0.09920534301235015, 0.08908541243837541, 0.08258074100118086, 0.07952102110328445, 0.07728398374046187, 0.07640040802309311, 0.07604687459856631, 0.07594730304666313, 0.07526631778981312, 0.0749419815748571, 0.07492964576525861]\n",
      "vaild_train:  [0.09653103179656543, 0.09138313358506331, 0.08571263617621018, 0.08330543138659917, 0.08159640374091956, 0.08040580325401746, 0.0797753009515313, 0.07962888278640233, 0.07816055717949684, 0.07856772231081358, 0.0780331796894853]\n",
      "Epoch [12/100], Train Loss: 0.0746, Valid Loss: 0.0775\n",
      "loss_train:  [0.09920534301235015, 0.08908541243837541, 0.08258074100118086, 0.07952102110328445, 0.07728398374046187, 0.07640040802309311, 0.07604687459856631, 0.07594730304666313, 0.07526631778981312, 0.0749419815748571, 0.07492964576525861, 0.0746184763060995]\n",
      "vaild_train:  [0.09653103179656543, 0.09138313358506331, 0.08571263617621018, 0.08330543138659917, 0.08159640374091956, 0.08040580325401746, 0.0797753009515313, 0.07962888278640233, 0.07816055717949684, 0.07856772231081358, 0.0780331796894853, 0.0774751718944082]\n",
      "Epoch [13/100], Train Loss: 0.0744, Valid Loss: 0.0775\n",
      "loss_train:  [0.09920534301235015, 0.08908541243837541, 0.08258074100118086, 0.07952102110328445, 0.07728398374046187, 0.07640040802309311, 0.07604687459856631, 0.07594730304666313, 0.07526631778981312, 0.0749419815748571, 0.07492964576525861, 0.0746184763060995, 0.07436384603560689]\n",
      "vaild_train:  [0.09653103179656543, 0.09138313358506331, 0.08571263617621018, 0.08330543138659917, 0.08159640374091956, 0.08040580325401746, 0.0797753009515313, 0.07962888278640233, 0.07816055717949684, 0.07856772231081358, 0.0780331796894853, 0.0774751718944082, 0.07754032256511542]\n",
      "Epoch [14/100], Train Loss: 0.0740, Valid Loss: 0.0772\n",
      "loss_train:  [0.09920534301235015, 0.08908541243837541, 0.08258074100118086, 0.07952102110328445, 0.07728398374046187, 0.07640040802309311, 0.07604687459856631, 0.07594730304666313, 0.07526631778981312, 0.0749419815748571, 0.07492964576525861, 0.0746184763060995, 0.07436384603560689, 0.07396601196154054]\n",
      "vaild_train:  [0.09653103179656543, 0.09138313358506331, 0.08571263617621018, 0.08330543138659917, 0.08159640374091956, 0.08040580325401746, 0.0797753009515313, 0.07962888278640233, 0.07816055717949684, 0.07856772231081358, 0.0780331796894853, 0.0774751718944082, 0.07754032256511542, 0.07715882590183845]\n",
      "Epoch [15/100], Train Loss: 0.0740, Valid Loss: 0.0770\n",
      "loss_train:  [0.09920534301235015, 0.08908541243837541, 0.08258074100118086, 0.07952102110328445, 0.07728398374046187, 0.07640040802309311, 0.07604687459856631, 0.07594730304666313, 0.07526631778981312, 0.0749419815748571, 0.07492964576525861, 0.0746184763060995, 0.07436384603560689, 0.07396601196154054, 0.07404848310243653]\n",
      "vaild_train:  [0.09653103179656543, 0.09138313358506331, 0.08571263617621018, 0.08330543138659917, 0.08159640374091956, 0.08040580325401746, 0.0797753009515313, 0.07962888278640233, 0.07816055717949684, 0.07856772231081358, 0.0780331796894853, 0.0774751718944082, 0.07754032256511542, 0.07715882590183845, 0.07700063481640357]\n",
      "Epoch [16/100], Train Loss: 0.0743, Valid Loss: 0.0770\n",
      "loss_train:  [0.09920534301235015, 0.08908541243837541, 0.08258074100118086, 0.07952102110328445, 0.07728398374046187, 0.07640040802309311, 0.07604687459856631, 0.07594730304666313, 0.07526631778981312, 0.0749419815748571, 0.07492964576525861, 0.0746184763060995, 0.07436384603560689, 0.07396601196154054, 0.07404848310243653, 0.07429902291441538]\n",
      "vaild_train:  [0.09653103179656543, 0.09138313358506331, 0.08571263617621018, 0.08330543138659917, 0.08159640374091956, 0.08040580325401746, 0.0797753009515313, 0.07962888278640233, 0.07816055717949684, 0.07856772231081358, 0.0780331796894853, 0.0774751718944082, 0.07754032256511542, 0.07715882590183845, 0.07700063481640357, 0.07704406088361374]\n",
      "Epoch [17/100], Train Loss: 0.0743, Valid Loss: 0.0772\n",
      "loss_train:  [0.09920534301235015, 0.08908541243837541, 0.08258074100118086, 0.07952102110328445, 0.07728398374046187, 0.07640040802309311, 0.07604687459856631, 0.07594730304666313, 0.07526631778981312, 0.0749419815748571, 0.07492964576525861, 0.0746184763060995, 0.07436384603560689, 0.07396601196154054, 0.07404848310243653, 0.07429902291441538, 0.07433087120214141]\n",
      "vaild_train:  [0.09653103179656543, 0.09138313358506331, 0.08571263617621018, 0.08330543138659917, 0.08159640374091956, 0.08040580325401746, 0.0797753009515313, 0.07962888278640233, 0.07816055717949684, 0.07856772231081358, 0.0780331796894853, 0.0774751718944082, 0.07754032256511542, 0.07715882590183845, 0.07700063481640357, 0.07704406088361374, 0.07717303179491025]\n",
      "Epoch [18/100], Train Loss: 0.0740, Valid Loss: 0.0765\n",
      "loss_train:  [0.09920534301235015, 0.08908541243837541, 0.08258074100118086, 0.07952102110328445, 0.07728398374046187, 0.07640040802309311, 0.07604687459856631, 0.07594730304666313, 0.07526631778981312, 0.0749419815748571, 0.07492964576525861, 0.0746184763060995, 0.07436384603560689, 0.07396601196154054, 0.07404848310243653, 0.07429902291441538, 0.07433087120214141, 0.07397668314985482]\n",
      "vaild_train:  [0.09653103179656543, 0.09138313358506331, 0.08571263617621018, 0.08330543138659917, 0.08159640374091956, 0.08040580325401746, 0.0797753009515313, 0.07962888278640233, 0.07816055717949684, 0.07856772231081358, 0.0780331796894853, 0.0774751718944082, 0.07754032256511542, 0.07715882590183845, 0.07700063481640357, 0.07704406088361374, 0.07717303179491025, 0.07647484154082261]\n",
      "Epoch [19/100], Train Loss: 0.0740, Valid Loss: 0.0763\n",
      "loss_train:  [0.09920534301235015, 0.08908541243837541, 0.08258074100118086, 0.07952102110328445, 0.07728398374046187, 0.07640040802309311, 0.07604687459856631, 0.07594730304666313, 0.07526631778981312, 0.0749419815748571, 0.07492964576525861, 0.0746184763060995, 0.07436384603560689, 0.07396601196154054, 0.07404848310243653, 0.07429902291441538, 0.07433087120214141, 0.07397668314985482, 0.07400760239506342]\n",
      "vaild_train:  [0.09653103179656543, 0.09138313358506331, 0.08571263617621018, 0.08330543138659917, 0.08159640374091956, 0.08040580325401746, 0.0797753009515313, 0.07962888278640233, 0.07816055717949684, 0.07856772231081358, 0.0780331796894853, 0.0774751718944082, 0.07754032256511542, 0.07715882590183845, 0.07700063481640357, 0.07704406088361374, 0.07717303179491025, 0.07647484154082261, 0.07628355698230174]\n",
      "Epoch [20/100], Train Loss: 0.0741, Valid Loss: 0.0770\n",
      "Early stopped at epoch 19, train loss stop improving\n",
      "Training finished\n",
      "728.2675793170929\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "es_cnt = 0\n",
    "es_thres = 5\n",
    "prev_train_loss = float('inf')\n",
    "loss_train = []\n",
    "loss_vaild = []\n",
    "num_epochs = 100 # 总训练轮数\n",
    "#num_batch_train = 0\n",
    "for epoch in range(num_epochs):\n",
    "  #train_bar = tqdm(train_loader)\n",
    "  train_loss = 0.0\n",
    "  \n",
    "  for i , (batch) in enumerate(train_loader):\n",
    "\n",
    "    # 数据转到device\n",
    "    train_batch = batch[0].to(device)\n",
    "    \n",
    "    # 训练步骤  \n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(train_batch)\n",
    "    loss = criterion(outputs, train_batch)\n",
    "    loss.backward() \n",
    "    optimizer.step()\n",
    "    \n",
    "    train_loss += loss.item()\n",
    "    #num_batch_train +=1\n",
    "  #train_loss除以所有bacth个数\n",
    "  train_loss = train_loss/(np.ceil(train.size(0)/batch_size1))\n",
    "  loss_train.append(train_loss)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "  # 验证\n",
    "  valid_loss = 0.0\n",
    "  #num_batch_vaild = 0\n",
    "  with torch.no_grad():\n",
    "    for i , (batch) in enumerate(vaild_loader):\n",
    "    #for batch in vaild_loader:\n",
    "    \n",
    "      val_batch = batch[0].to(device)\n",
    "      \n",
    "      outputs = model(val_batch)\n",
    "      loss = criterion(outputs, val_batch)\n",
    "      valid_loss += loss.item()\n",
    "      #num_batch_vaild += 1\n",
    "    valid_loss = valid_loss/(np.ceil(vaild.size(0)/batch_size1))\n",
    "    loss_vaild.append(valid_loss)\n",
    "    print(\"Epoch [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}\".format(epoch+1, num_epochs, train_loss, valid_loss))\n",
    "\n",
    "    \n",
    "    # Early stopping\n",
    "    if train_loss - prev_train_loss >= 0:\n",
    "        es_cnt += 1\n",
    "    else:\n",
    "        #es_cnt = 0\n",
    "        pass\n",
    "\n",
    "    if es_cnt >= es_thres:\n",
    "        print(f\"Early stopped at epoch {epoch}, train loss stop improving\")\n",
    "        break  \n",
    "\n",
    "    prev_train_loss = train_loss\n",
    "  print('loss_train: ', loss_train)\n",
    "  print('vaild_train: ',loss_vaild)          \n",
    "print(\"Training finished\")\n",
    "current_time = time.time()\n",
    "time_sum = current_time-start_time\n",
    "print(time_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train = pd.DataFrame(loss_train)\n",
    "loss_vaild = pd.DataFrame(loss_vaild)\n",
    "\n",
    "loss = pd.concat([loss_train,loss_vaild],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.columns = ['train_loss','vaild_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), r'.\\model_2dsyn1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([33142, 2304])\n"
     ]
    }
   ],
   "source": [
    "model = AutoEncoder(w1*w2,w1*w2).to(device)\n",
    "data = data.to(device)\n",
    "model.load_state_dict(torch.load(r'.\\model_2dsyn1.pth'))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(data)\n",
    "    #loss = criterion(output, data)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output.cpu()\n",
    "output = output.numpy()\n",
    "output = pd.DataFrame(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.to_csv(r'loss_2dsyn1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv(r'./output_2dsyn1.csv',index=None,header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss_0 = []\n",
    "#start_time = time.time()\n",
    "#es_cnt = 0\n",
    "#es_thres = 5\n",
    "#prev_train_loss = float('inf')\n",
    "#loss_train = []\n",
    "#loss_vaild = []\n",
    "#num_epochs = 100 # 总训练轮数\n",
    "##num_batch_train = 0\n",
    "#for epoch in range(num_epochs):\n",
    "#  #train_bar = tqdm(train_loader)\n",
    "#  train_loss = 0.0\n",
    "#  \n",
    "#  for i , (batch) in enumerate(train_loader):\n",
    "#\n",
    "#    # 数据转到device\n",
    "#    train_batch = batch[0].to(device)\n",
    "#    \n",
    "#    # 训练步骤  \n",
    "#    optimizer.zero_grad()\n",
    "#    outputs = model(train_batch)\n",
    "#    loss = criterion(outputs, train_batch)\n",
    "#    loss.backward() \n",
    "#    optimizer.step()\n",
    "#\n",
    "#    loss_0.append(loss.item())\n",
    "#\n",
    "#    train_loss += loss.item()\n",
    "#    #num_batch_train +=1\n",
    "#  #train_loss除以所有bacth个数\n",
    "#  train_loss = train_loss/(np.ceil(train.size(0)/batch_size1))\n",
    "#  loss_train.append(train_loss)\n",
    "#    \n",
    "#\n",
    "#\n",
    "#\n",
    "#  # 验证\n",
    "#  valid_loss = 0.0\n",
    "#  #num_batch_vaild = 0\n",
    "#  with torch.no_grad():\n",
    "#    for i , (batch) in enumerate(vaild_loader):\n",
    "#    #for batch in vaild_loader:\n",
    "#    \n",
    "#      val_batch = batch[0].to(device)\n",
    "#      \n",
    "#      outputs = model(val_batch)\n",
    "#      loss = criterion(outputs, val_batch)\n",
    "#      valid_loss += loss.item()\n",
    "#      #num_batch_vaild += 1\n",
    "#    valid_loss = valid_loss/(np.ceil(vaild.size(0)/batch_size1))\n",
    "#    loss_vaild.append(valid_loss)\n",
    "#    print(\"Epoch [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}\".format(epoch+1, num_epochs, train_loss, valid_loss))\n",
    "#\n",
    "#    \n",
    "#    # Early stopping\n",
    "#    if train_loss - prev_train_loss >= 0:\n",
    "#        es_cnt += 1\n",
    "#    else:\n",
    "#        #es_cnt = 0\n",
    "#        pass\n",
    "#\n",
    "#    if es_cnt >= es_thres:\n",
    "#        print(f\"Early stopped at epoch {epoch}, train loss stop improving\")\n",
    "#        break  \n",
    "#    \n",
    "#\n",
    "#\n",
    "##   if epoch == 10:\n",
    "##     torch.save(model.state_dict(), r'.\\model_epochs10.pth')\n",
    "##   elif epoch == 20:\n",
    "##     torch.save(model.state_dict(), r'.\\model_epochs20.pth')\n",
    "##   elif epoch == 30:\n",
    "##     torch.save(model.state_dict(), r'.\\model_epochs30.pth')\n",
    "##   elif epoch == 40:\n",
    "##     torch.save(model.state_dict(), r'.\\model_epochs40.pth')\n",
    "##   elif epoch == 50:\n",
    "##     torch.save(model.state_dict(), r'.\\model_epochs50.pth')\n",
    "##   elif epoch == 60:\n",
    "##     torch.save(model.state_dict(), r'.\\model_epochs60.pth')\n",
    "##   elif epoch == 70:\n",
    "##     torch.save(model.state_dict(), r'.\\model_epochs70.pth')\n",
    "##   elif epoch == 80:\n",
    "##     torch.save(model.state_dict(), r'.\\model_epochs80.pth')\n",
    "##   elif epoch == 90:\n",
    "##     torch.save(model.state_dict(), r'.\\model_epochs90.pth')\n",
    "##   elif epoch == 100:\n",
    "##     torch.save(model.state_dict(), r'.\\model_epochs100.pth')\n",
    "##   else:\n",
    "##       pass\n",
    "#\n",
    "#    prev_train_loss = train_loss\n",
    "#  print('loss_train: ', loss_train)\n",
    "#  print('vaild_train: ',loss_vaild)          \n",
    "#print(\"Training finished\")\n",
    "#current_time = time.time()\n",
    "#time_sum = current_time-start_time\n",
    "#print(time_sum)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
