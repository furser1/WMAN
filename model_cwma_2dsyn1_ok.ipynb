{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#import tushare as ts\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from torch.utils.data import TensorDataset\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import scipy.io as scio\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_noisy = r'Input_Patches_2Dsyn11.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(path_noisy, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2294</th>\n",
       "      <th>2295</th>\n",
       "      <th>2296</th>\n",
       "      <th>2297</th>\n",
       "      <th>2298</th>\n",
       "      <th>2299</th>\n",
       "      <th>2300</th>\n",
       "      <th>2301</th>\n",
       "      <th>2302</th>\n",
       "      <th>2303</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.285655</td>\n",
       "      <td>-0.094759</td>\n",
       "      <td>0.059033</td>\n",
       "      <td>0.082095</td>\n",
       "      <td>-0.169731</td>\n",
       "      <td>-0.141456</td>\n",
       "      <td>-0.065269</td>\n",
       "      <td>-0.101775</td>\n",
       "      <td>-0.305867</td>\n",
       "      <td>0.142491</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010872</td>\n",
       "      <td>-0.494745</td>\n",
       "      <td>0.326118</td>\n",
       "      <td>0.023124</td>\n",
       "      <td>0.054619</td>\n",
       "      <td>0.011072</td>\n",
       "      <td>0.328060</td>\n",
       "      <td>-0.203638</td>\n",
       "      <td>-0.329366</td>\n",
       "      <td>0.017071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.034454</td>\n",
       "      <td>0.024164</td>\n",
       "      <td>-0.350245</td>\n",
       "      <td>0.085406</td>\n",
       "      <td>0.066838</td>\n",
       "      <td>0.103323</td>\n",
       "      <td>0.183920</td>\n",
       "      <td>0.196916</td>\n",
       "      <td>0.400856</td>\n",
       "      <td>-0.097729</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.083210</td>\n",
       "      <td>-0.213278</td>\n",
       "      <td>0.398580</td>\n",
       "      <td>0.199259</td>\n",
       "      <td>0.193139</td>\n",
       "      <td>-0.182990</td>\n",
       "      <td>-0.151285</td>\n",
       "      <td>0.054522</td>\n",
       "      <td>0.065290</td>\n",
       "      <td>0.144964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.079837</td>\n",
       "      <td>-0.129883</td>\n",
       "      <td>-0.157144</td>\n",
       "      <td>-0.011358</td>\n",
       "      <td>0.043089</td>\n",
       "      <td>-0.154546</td>\n",
       "      <td>-0.025915</td>\n",
       "      <td>0.608872</td>\n",
       "      <td>0.222907</td>\n",
       "      <td>-0.214785</td>\n",
       "      <td>...</td>\n",
       "      <td>0.482898</td>\n",
       "      <td>0.356052</td>\n",
       "      <td>-0.223782</td>\n",
       "      <td>-0.116979</td>\n",
       "      <td>0.066285</td>\n",
       "      <td>-0.001133</td>\n",
       "      <td>-0.052318</td>\n",
       "      <td>-0.062070</td>\n",
       "      <td>-0.075138</td>\n",
       "      <td>0.269355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.239533</td>\n",
       "      <td>-0.248196</td>\n",
       "      <td>0.082898</td>\n",
       "      <td>-0.101518</td>\n",
       "      <td>0.311064</td>\n",
       "      <td>-0.005159</td>\n",
       "      <td>0.091698</td>\n",
       "      <td>0.111151</td>\n",
       "      <td>-0.051184</td>\n",
       "      <td>-0.204132</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.314867</td>\n",
       "      <td>-0.242084</td>\n",
       "      <td>-0.071824</td>\n",
       "      <td>-0.056087</td>\n",
       "      <td>0.134982</td>\n",
       "      <td>0.389893</td>\n",
       "      <td>0.016835</td>\n",
       "      <td>-0.232038</td>\n",
       "      <td>0.160488</td>\n",
       "      <td>0.108543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.625598</td>\n",
       "      <td>0.099131</td>\n",
       "      <td>0.100790</td>\n",
       "      <td>0.496220</td>\n",
       "      <td>-0.033520</td>\n",
       "      <td>1.222121</td>\n",
       "      <td>0.216445</td>\n",
       "      <td>0.203567</td>\n",
       "      <td>0.363440</td>\n",
       "      <td>-0.184805</td>\n",
       "      <td>...</td>\n",
       "      <td>0.282989</td>\n",
       "      <td>0.312111</td>\n",
       "      <td>0.119921</td>\n",
       "      <td>-0.299165</td>\n",
       "      <td>0.060922</td>\n",
       "      <td>0.158843</td>\n",
       "      <td>-0.100142</td>\n",
       "      <td>0.015174</td>\n",
       "      <td>0.059186</td>\n",
       "      <td>0.354470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33137</th>\n",
       "      <td>0.016760</td>\n",
       "      <td>0.095579</td>\n",
       "      <td>0.179706</td>\n",
       "      <td>0.077812</td>\n",
       "      <td>0.106669</td>\n",
       "      <td>-0.449320</td>\n",
       "      <td>0.047308</td>\n",
       "      <td>-0.429691</td>\n",
       "      <td>0.127659</td>\n",
       "      <td>0.050212</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036615</td>\n",
       "      <td>-0.164099</td>\n",
       "      <td>-0.182914</td>\n",
       "      <td>-0.083881</td>\n",
       "      <td>0.217836</td>\n",
       "      <td>-0.162975</td>\n",
       "      <td>-0.216453</td>\n",
       "      <td>0.528084</td>\n",
       "      <td>0.151361</td>\n",
       "      <td>-0.348705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33138</th>\n",
       "      <td>-0.231164</td>\n",
       "      <td>-0.173054</td>\n",
       "      <td>-0.117294</td>\n",
       "      <td>0.149689</td>\n",
       "      <td>-0.097406</td>\n",
       "      <td>0.342493</td>\n",
       "      <td>-0.011129</td>\n",
       "      <td>-0.307579</td>\n",
       "      <td>-0.147544</td>\n",
       "      <td>0.015597</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.239727</td>\n",
       "      <td>0.168824</td>\n",
       "      <td>0.329842</td>\n",
       "      <td>-0.226346</td>\n",
       "      <td>0.250965</td>\n",
       "      <td>0.353435</td>\n",
       "      <td>0.181593</td>\n",
       "      <td>0.019157</td>\n",
       "      <td>-0.116774</td>\n",
       "      <td>-0.007252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33139</th>\n",
       "      <td>-0.078532</td>\n",
       "      <td>0.020383</td>\n",
       "      <td>-0.073423</td>\n",
       "      <td>0.043979</td>\n",
       "      <td>-0.008138</td>\n",
       "      <td>0.102078</td>\n",
       "      <td>-0.326204</td>\n",
       "      <td>-0.343582</td>\n",
       "      <td>-0.218178</td>\n",
       "      <td>0.136061</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.076938</td>\n",
       "      <td>0.521511</td>\n",
       "      <td>0.112031</td>\n",
       "      <td>-0.056174</td>\n",
       "      <td>-0.225504</td>\n",
       "      <td>0.015828</td>\n",
       "      <td>-0.083210</td>\n",
       "      <td>0.146706</td>\n",
       "      <td>0.143406</td>\n",
       "      <td>0.167758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33140</th>\n",
       "      <td>-0.231931</td>\n",
       "      <td>0.061216</td>\n",
       "      <td>-0.168617</td>\n",
       "      <td>-0.094112</td>\n",
       "      <td>-0.081586</td>\n",
       "      <td>-0.113479</td>\n",
       "      <td>0.063115</td>\n",
       "      <td>-0.027429</td>\n",
       "      <td>-0.097478</td>\n",
       "      <td>-0.018648</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.258984</td>\n",
       "      <td>-0.148469</td>\n",
       "      <td>-0.149496</td>\n",
       "      <td>0.053072</td>\n",
       "      <td>-0.309760</td>\n",
       "      <td>0.048369</td>\n",
       "      <td>-0.032407</td>\n",
       "      <td>-0.070822</td>\n",
       "      <td>-0.263769</td>\n",
       "      <td>0.302118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33141</th>\n",
       "      <td>-0.070338</td>\n",
       "      <td>-0.094341</td>\n",
       "      <td>-0.125153</td>\n",
       "      <td>0.073751</td>\n",
       "      <td>-0.048972</td>\n",
       "      <td>0.380870</td>\n",
       "      <td>0.041785</td>\n",
       "      <td>0.113918</td>\n",
       "      <td>0.198744</td>\n",
       "      <td>0.184822</td>\n",
       "      <td>...</td>\n",
       "      <td>0.094033</td>\n",
       "      <td>-0.317294</td>\n",
       "      <td>-0.320403</td>\n",
       "      <td>-0.067642</td>\n",
       "      <td>-0.287481</td>\n",
       "      <td>-0.359201</td>\n",
       "      <td>0.089425</td>\n",
       "      <td>-0.197743</td>\n",
       "      <td>0.105451</td>\n",
       "      <td>-0.025696</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33142 rows × 2304 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6     \\\n",
       "0     -0.285655 -0.094759  0.059033  0.082095 -0.169731 -0.141456 -0.065269   \n",
       "1     -0.034454  0.024164 -0.350245  0.085406  0.066838  0.103323  0.183920   \n",
       "2     -0.079837 -0.129883 -0.157144 -0.011358  0.043089 -0.154546 -0.025915   \n",
       "3      0.239533 -0.248196  0.082898 -0.101518  0.311064 -0.005159  0.091698   \n",
       "4      0.625598  0.099131  0.100790  0.496220 -0.033520  1.222121  0.216445   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "33137  0.016760  0.095579  0.179706  0.077812  0.106669 -0.449320  0.047308   \n",
       "33138 -0.231164 -0.173054 -0.117294  0.149689 -0.097406  0.342493 -0.011129   \n",
       "33139 -0.078532  0.020383 -0.073423  0.043979 -0.008138  0.102078 -0.326204   \n",
       "33140 -0.231931  0.061216 -0.168617 -0.094112 -0.081586 -0.113479  0.063115   \n",
       "33141 -0.070338 -0.094341 -0.125153  0.073751 -0.048972  0.380870  0.041785   \n",
       "\n",
       "           7         8         9     ...      2294      2295      2296  \\\n",
       "0     -0.101775 -0.305867  0.142491  ...  0.010872 -0.494745  0.326118   \n",
       "1      0.196916  0.400856 -0.097729  ... -0.083210 -0.213278  0.398580   \n",
       "2      0.608872  0.222907 -0.214785  ...  0.482898  0.356052 -0.223782   \n",
       "3      0.111151 -0.051184 -0.204132  ... -0.314867 -0.242084 -0.071824   \n",
       "4      0.203567  0.363440 -0.184805  ...  0.282989  0.312111  0.119921   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "33137 -0.429691  0.127659  0.050212  ... -0.036615 -0.164099 -0.182914   \n",
       "33138 -0.307579 -0.147544  0.015597  ... -0.239727  0.168824  0.329842   \n",
       "33139 -0.343582 -0.218178  0.136061  ... -0.076938  0.521511  0.112031   \n",
       "33140 -0.027429 -0.097478 -0.018648  ... -0.258984 -0.148469 -0.149496   \n",
       "33141  0.113918  0.198744  0.184822  ...  0.094033 -0.317294 -0.320403   \n",
       "\n",
       "           2297      2298      2299      2300      2301      2302      2303  \n",
       "0      0.023124  0.054619  0.011072  0.328060 -0.203638 -0.329366  0.017071  \n",
       "1      0.199259  0.193139 -0.182990 -0.151285  0.054522  0.065290  0.144964  \n",
       "2     -0.116979  0.066285 -0.001133 -0.052318 -0.062070 -0.075138  0.269355  \n",
       "3     -0.056087  0.134982  0.389893  0.016835 -0.232038  0.160488  0.108543  \n",
       "4     -0.299165  0.060922  0.158843 -0.100142  0.015174  0.059186  0.354470  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "33137 -0.083881  0.217836 -0.162975 -0.216453  0.528084  0.151361 -0.348705  \n",
       "33138 -0.226346  0.250965  0.353435  0.181593  0.019157 -0.116774 -0.007252  \n",
       "33139 -0.056174 -0.225504  0.015828 -0.083210  0.146706  0.143406  0.167758  \n",
       "33140  0.053072 -0.309760  0.048369 -0.032407 -0.070822 -0.263769  0.302118  \n",
       "33141 -0.067642 -0.287481 -0.359201  0.089425 -0.197743  0.105451 -0.025696  \n",
       "\n",
       "[33142 rows x 2304 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.astype(np.float32)\n",
    "data= torch.from_numpy(data.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([33142, 2304])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([26513, 2304])\n",
      "torch.Size([6629, 2304])\n"
     ]
    }
   ],
   "source": [
    "#将前80%作为训练集，后20%作为测试集\n",
    "train_size = int(len(data) * 0.8)\n",
    "train = data[:train_size]\n",
    "vaild = data[train_size:]\n",
    "print(train.shape)\n",
    "print(vaild.shape)\n",
    "batch_size1 = 64\n",
    "w1 = 48\n",
    "w2 = 48\n",
    "\n",
    "train_data= TensorDataset(train)\n",
    "vaild_data= TensorDataset(vaild)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data,\n",
    "                                           batch_size = 64,\n",
    "                                           shuffle = True)\n",
    "\n",
    "vaild_loader = torch.utils.data.DataLoader(vaild_data,\n",
    "                                          batch_size = 64,\n",
    "                                          shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义Fully connected (FC) block\n",
    "class FCB(nn.Module):\n",
    "    def __init__(self, input_size, output_size, dropout=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "        self.activation = nn.LeakyReLU()\n",
    "        self.bn = nn.BatchNorm1d(output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = self.activation(x) \n",
    "        x = self.bn(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PAB(nn.Module):  ##position attention block\n",
    "    def __init__(self, input_size, output_size, dropout=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fcb1 = FCB(input_size, output_size, dropout)\n",
    "        self.fcb2 = FCB(input_size, output_size, dropout)\n",
    "        self.fcb3 = FCB(input_size, output_size, dropout)\n",
    "        self.fcb4 = FCB(input_size, output_size, dropout)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.fcb1(x)\n",
    "        x2 = self.fcb2(x)\n",
    "        x3 = self.fcb3(x)\n",
    "        x4 = self.fcb4(x)\n",
    "        \n",
    "\n",
    "        x = x1*x2\n",
    "        x = self.softmax(x)\n",
    "        x = x*x3\n",
    "        x = x+x4\n",
    "        \n",
    "        return x   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, dropout=0.3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cov1d = nn.Conv1d(in_channels=1, out_channels= 64 ,kernel_size = 5, padding=0, stride=1)\n",
    "\n",
    "        self.pab1 = PAB(int(input_size-5+1), 128, dropout)\n",
    "        self.fcb1 = FCB(128, 128, dropout)\n",
    "\n",
    "        self.pab2 = PAB(128, 64, dropout)\n",
    "        self.fcb2 = FCB(64, 64, dropout)\n",
    "\n",
    "        self.pab3 = PAB(64, 32, dropout)\n",
    "        self.fcb3 = FCB(32, 32, dropout)\n",
    "\n",
    "        self.pab4 = PAB(32, 16, dropout)\n",
    "        self.fcb4 = FCB(16, 16, dropout)\n",
    "\n",
    "        self.pab5 = PAB(16, 8, dropout)\n",
    "        self.fcb5 = FCB(8, 8, dropout)\n",
    "\n",
    "        self.pab6 = PAB(8, 4, dropout)\n",
    "        self.fcb6 = FCB(4, 4, dropout)\n",
    "        \n",
    "        self.fcb7 = FCB(4, 4, dropout)\n",
    "        self.pab7 = PAB(4, 4, dropout)\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x.reshape(x.shape[0],1,x.shape[1])\n",
    "        x = self.cov1d(x)\n",
    "        x = x.permute(0,2,1)\n",
    "        x = nn.AdaptiveAvgPool1d(1)(x)\n",
    "        x = x.squeeze()\n",
    "\n",
    "        x1 = self.pab1(x)\n",
    "        x13 = self.fcb1(x1)\n",
    "\n",
    "        x2 = self.pab2(x1)\n",
    "        x12 = self.fcb2(x2)\n",
    "\n",
    "        x3 = self.pab3(x2)\n",
    "        x11 = self.fcb3(x3)\n",
    "\n",
    "        x4 = self.pab4(x3)\n",
    "        x10 = self.fcb4(x4)\n",
    "\n",
    "        x5 = self.pab5(x4)\n",
    "        x9 = self.fcb5(x5)\n",
    "\n",
    "        x6 = self.pab6(x5)\n",
    "        x8 = self.fcb6(x6)\n",
    "\n",
    "        x7 = self.fcb7(x6)\n",
    "        x7 = self.pab7(x7)\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        return x7,x8,x9,x10,x11,x12,x13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomActivation(torch.autograd.Function):    \n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        # 计算正向传播结果\n",
    "        ctx.save_for_backward(x)\n",
    "        y = 12/3 * (x - torch.tanh(x)) * torch.cos(x/2)\n",
    "        return y\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # 计算反向传播结果\n",
    "        x, = ctx.saved_tensors\n",
    "        grad_x = grad_output * (12/3 * ((torch.cos(x/2)*(1 - torch.pow(torch.cosh(x), -2)))+ (x - torch.tanh(x))*(-0.5*torch.sin(x/2))))\n",
    "                                       \n",
    "        return grad_x\n",
    "    \n",
    "class CustomActivationModule(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        return CustomActivation.apply(x)  \n",
    "    \n",
    "\n",
    "activation = CustomActivationModule()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size, dropout=0.3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pab1 = PAB(8, 8, dropout)\n",
    "        self.pab2 = PAB(16, 16, dropout)\n",
    "        self.pab3 = PAB(32, 32, dropout)\n",
    "        self.pab4 = PAB(64, 64, dropout)\n",
    "        self.pab5 = PAB(128, 128, dropout)\n",
    "        #self.pab5 = PAB(256, 256, dropout)\n",
    "\n",
    "        self.fc = nn.Linear(256,output_size)\n",
    "        #self.activation = nn.Identity()\n",
    "        self.activation = activation\n",
    "\n",
    "\n",
    "    def forward(self, x7,x8,x9,x10,x11,x12,x13):\n",
    "        \n",
    "        x = torch.cat((x7,x8),dim=1)\n",
    "        x = self.pab1(x)\n",
    "        x = torch.cat((x,x9),dim=1)\n",
    "        x = self.pab2(x)\n",
    "        x = torch.cat((x,x10),dim=1)\n",
    "        x = self.pab3(x)\n",
    "        x = torch.cat((x,x11),dim=1)\n",
    "        x = self.pab4(x)\n",
    "        x = torch.cat((x,x12),dim=1)\n",
    "        x = self.pab5(x)\n",
    "        x = torch.cat((x,x13),dim=1)\n",
    "        x = self.fc(x)\n",
    "        #x.requires_grad_(True)\n",
    "        x = self.activation(x)\n",
    "        #x.sum().backward()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self,input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(input_size)\n",
    "        self.decoder = Decoder(output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x7,x8,x9,x10,x11,x12,x13= self.encoder(x)\n",
    "        x = self.decoder(x7,x8,x9,x10,x11,x12,x13)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(nn.Module):\n",
    "    def __init__(self,delta,r):\n",
    "        super().__init__()\n",
    "        self.delta = delta\n",
    "        self.r = r\n",
    "    def forward(self,input,output):\n",
    "        N = input-output\n",
    "        temp = output*N\n",
    "        temp1 = data.shape[1]*(temp.sum(1))\n",
    "        temp2 = (output.sum(1))*(N.sum(1))\n",
    "        temp3 = torch.pow(data.shape[1]*((output**2).sum(1)) - ((output.sum(1))**2),1/2)\n",
    "        temp4 = torch.pow(data.shape[1]*((N**2).sum(1))-((N.sum(1))**2),1/2) \n",
    "        loss = (temp1-temp2)/(temp3*temp4)\n",
    "\n",
    "        a = torch.min(loss)\n",
    "        b = torch.mean(1 - torch.exp(-0.5 * torch.pow((N / self.delta),2)))\n",
    "        \n",
    "        return self.r*a+(1-self.r)*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss0(nn.Module):\n",
    "    def __init__(self,delta,r):\n",
    "        super().__init__()\n",
    "        self.delta = delta\n",
    "        self.r = r\n",
    "    def forward(self,input,output):\n",
    "        N = input-output\n",
    "        b = torch.mean(1 - torch.exp(-0.5 * torch.pow((N / self.delta),2)))\n",
    "        K = output.shape[0]\n",
    "        y2_mean = torch.mean(output)\n",
    "        n2_mean = torch.mean(N)\n",
    "        covariance = 0.0\n",
    "        variance_y2 = 0.0\n",
    "        variance_n2 = 0.0\n",
    "        for j in range(K):\n",
    "            y2_sample = output[j, :]\n",
    "            n2_sample = N[j, :]\n",
    "            covariance += ((y2_sample - y2_mean) * (n2_sample - n2_mean))\n",
    "            variance_y2 += ((y2_sample - y2_mean) ** 2)\n",
    "            variance_n2 += ((n2_sample - n2_mean) ** 2)\n",
    "            lcc_value = (covariance / torch.sqrt(variance_y2 * variance_n2)) ** 2\n",
    "            lcc_value = torch.mean(lcc_value)\n",
    "        return self.r*lcc_value+(1-self.r)*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") \n",
    "model = AutoEncoder(w1*w2,w1*w2).to(device)\n",
    "#将模型转移到GPU上\n",
    "#criterion = MeanHuberLoss(delta=1.3)\n",
    "#criterion = WelschLoss(delta=0.5)\n",
    "#criterion = Loss0(delta=0.46,r=0.05)#0.5 and 0.2,SNR:-8dB ok\n",
    "criterion = Loss0(delta=0.42,r=0.01)\n",
    "#criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Train Loss: 0.1740, Valid Loss: 0.1816\n",
      "loss_train:  [0.173994489822043]\n",
      "vaild_train:  [0.18155935077139965]\n",
      "Epoch [2/100], Train Loss: 0.1593, Valid Loss: 0.1625\n",
      "loss_train:  [0.173994489822043, 0.15933115083769142]\n",
      "vaild_train:  [0.18155935077139965, 0.16249542239193732]\n",
      "Epoch [3/100], Train Loss: 0.1542, Valid Loss: 0.1580\n",
      "loss_train:  [0.173994489822043, 0.15933115083769142, 0.15416382411158228]\n",
      "vaild_train:  [0.18155935077139965, 0.16249542239193732, 0.15801524098675984]\n",
      "Epoch [4/100], Train Loss: 0.1514, Valid Loss: 0.1557\n",
      "loss_train:  [0.173994489822043, 0.15933115083769142, 0.15416382411158228, 0.15139235685388727]\n",
      "vaild_train:  [0.18155935077139965, 0.16249542239193732, 0.15801524098675984, 0.15568859731921783]\n",
      "Epoch [5/100], Train Loss: 0.1504, Valid Loss: 0.1555\n",
      "loss_train:  [0.173994489822043, 0.15933115083769142, 0.15416382411158228, 0.15139235685388727, 0.15036228956228279]\n",
      "vaild_train:  [0.18155935077139965, 0.16249542239193732, 0.15801524098675984, 0.15568859731921783, 0.15553964058367106]\n",
      "Epoch [6/100], Train Loss: 0.1501, Valid Loss: 0.1549\n",
      "loss_train:  [0.173994489822043, 0.15933115083769142, 0.15416382411158228, 0.15139235685388727, 0.15036228956228279, 0.1500927410570972]\n",
      "vaild_train:  [0.18155935077139965, 0.16249542239193732, 0.15801524098675984, 0.15568859731921783, 0.15553964058367106, 0.15485928723445305]\n",
      "Epoch [7/100], Train Loss: 0.1497, Valid Loss: 0.1550\n",
      "loss_train:  [0.173994489822043, 0.15933115083769142, 0.15416382411158228, 0.15139235685388727, 0.15036228956228279, 0.1500927410570972, 0.14965679426509212]\n",
      "vaild_train:  [0.18155935077139965, 0.16249542239193732, 0.15801524098675984, 0.15568859731921783, 0.15553964058367106, 0.15485928723445305, 0.15498607620023763]\n",
      "Epoch [8/100], Train Loss: 0.1490, Valid Loss: 0.1525\n",
      "loss_train:  [0.173994489822043, 0.15933115083769142, 0.15416382411158228, 0.15139235685388727, 0.15036228956228279, 0.1500927410570972, 0.14965679426509212, 0.14900887260235937]\n",
      "vaild_train:  [0.18155935077139965, 0.16249542239193732, 0.15801524098675984, 0.15568859731921783, 0.15553964058367106, 0.15485928723445305, 0.15498607620023763, 0.15245926981935135]\n",
      "Epoch [9/100], Train Loss: 0.1489, Valid Loss: 0.1543\n",
      "loss_train:  [0.173994489822043, 0.15933115083769142, 0.15416382411158228, 0.15139235685388727, 0.15036228956228279, 0.1500927410570972, 0.14965679426509212, 0.14900887260235937, 0.14885464619441205]\n",
      "vaild_train:  [0.18155935077139965, 0.16249542239193732, 0.15801524098675984, 0.15568859731921783, 0.15553964058367106, 0.15485928723445305, 0.15498607620023763, 0.15245926981935135, 0.1543110369776304]\n",
      "Epoch [10/100], Train Loss: 0.1483, Valid Loss: 0.1529\n",
      "loss_train:  [0.173994489822043, 0.15933115083769142, 0.15416382411158228, 0.15139235685388727, 0.15036228956228279, 0.1500927410570972, 0.14965679426509212, 0.14900887260235937, 0.14885464619441205, 0.14828455627682696]\n",
      "vaild_train:  [0.18155935077139965, 0.16249542239193732, 0.15801524098675984, 0.15568859731921783, 0.15553964058367106, 0.15485928723445305, 0.15498607620023763, 0.15245926981935135, 0.1543110369776304, 0.15292442260453334]\n",
      "Epoch [11/100], Train Loss: 0.1475, Valid Loss: 0.1532\n",
      "loss_train:  [0.173994489822043, 0.15933115083769142, 0.15416382411158228, 0.15139235685388727, 0.15036228956228279, 0.1500927410570972, 0.14965679426509212, 0.14900887260235937, 0.14885464619441205, 0.14828455627682696, 0.1474684160875987]\n",
      "vaild_train:  [0.18155935077139965, 0.16249542239193732, 0.15801524098675984, 0.15568859731921783, 0.15553964058367106, 0.15485928723445305, 0.15498607620023763, 0.15245926981935135, 0.1543110369776304, 0.15292442260453334, 0.1531637143343687]\n",
      "Epoch [12/100], Train Loss: 0.1481, Valid Loss: 0.1532\n",
      "loss_train:  [0.173994489822043, 0.15933115083769142, 0.15416382411158228, 0.15139235685388727, 0.15036228956228279, 0.1500927410570972, 0.14965679426509212, 0.14900887260235937, 0.14885464619441205, 0.14828455627682696, 0.1474684160875987, 0.1481493124760777]\n",
      "vaild_train:  [0.18155935077139965, 0.16249542239193732, 0.15801524098675984, 0.15568859731921783, 0.15553964058367106, 0.15485928723445305, 0.15498607620023763, 0.15245926981935135, 0.1543110369776304, 0.15292442260453334, 0.1531637143343687, 0.1532339735959585]\n",
      "Epoch [13/100], Train Loss: 0.1491, Valid Loss: 0.1533\n",
      "loss_train:  [0.173994489822043, 0.15933115083769142, 0.15416382411158228, 0.15139235685388727, 0.15036228956228279, 0.1500927410570972, 0.14965679426509212, 0.14900887260235937, 0.14885464619441205, 0.14828455627682696, 0.1474684160875987, 0.1481493124760777, 0.14913317478564847]\n",
      "vaild_train:  [0.18155935077139965, 0.16249542239193732, 0.15801524098675984, 0.15568859731921783, 0.15553964058367106, 0.15485928723445305, 0.15498607620023763, 0.15245926981935135, 0.1543110369776304, 0.15292442260453334, 0.1531637143343687, 0.1532339735959585, 0.15330183620636278]\n",
      "Epoch [14/100], Train Loss: 0.1472, Valid Loss: 0.1523\n",
      "loss_train:  [0.173994489822043, 0.15933115083769142, 0.15416382411158228, 0.15139235685388727, 0.15036228956228279, 0.1500927410570972, 0.14965679426509212, 0.14900887260235937, 0.14885464619441205, 0.14828455627682696, 0.1474684160875987, 0.1481493124760777, 0.14913317478564847, 0.14721806200153856]\n",
      "vaild_train:  [0.18155935077139965, 0.16249542239193732, 0.15801524098675984, 0.15568859731921783, 0.15553964058367106, 0.15485928723445305, 0.15498607620023763, 0.15245926981935135, 0.1543110369776304, 0.15292442260453334, 0.1531637143343687, 0.1532339735959585, 0.15330183620636278, 0.15225366717920855]\n",
      "Epoch [15/100], Train Loss: 0.1478, Valid Loss: 0.1523\n",
      "loss_train:  [0.173994489822043, 0.15933115083769142, 0.15416382411158228, 0.15139235685388727, 0.15036228956228279, 0.1500927410570972, 0.14965679426509212, 0.14900887260235937, 0.14885464619441205, 0.14828455627682696, 0.1474684160875987, 0.1481493124760777, 0.14913317478564847, 0.14721806200153856, 0.14780875932739443]\n",
      "vaild_train:  [0.18155935077139965, 0.16249542239193732, 0.15801524098675984, 0.15568859731921783, 0.15553964058367106, 0.15485928723445305, 0.15498607620023763, 0.15245926981935135, 0.1543110369776304, 0.15292442260453334, 0.1531637143343687, 0.1532339735959585, 0.15330183620636278, 0.15225366717920855, 0.1523370732768224]\n",
      "Epoch [16/100], Train Loss: 0.1492, Valid Loss: 0.1560\n",
      "loss_train:  [0.173994489822043, 0.15933115083769142, 0.15416382411158228, 0.15139235685388727, 0.15036228956228279, 0.1500927410570972, 0.14965679426509212, 0.14900887260235937, 0.14885464619441205, 0.14828455627682696, 0.1474684160875987, 0.1481493124760777, 0.14913317478564847, 0.14721806200153856, 0.14780875932739443, 0.14916685219988765]\n",
      "vaild_train:  [0.18155935077139965, 0.16249542239193732, 0.15801524098675984, 0.15568859731921783, 0.15553964058367106, 0.15485928723445305, 0.15498607620023763, 0.15245926981935135, 0.1543110369776304, 0.15292442260453334, 0.1531637143343687, 0.1532339735959585, 0.15330183620636278, 0.15225366717920855, 0.1523370732768224, 0.15595384142719781]\n",
      "Epoch [17/100], Train Loss: 0.1490, Valid Loss: 0.1563\n",
      "loss_train:  [0.173994489822043, 0.15933115083769142, 0.15416382411158228, 0.15139235685388727, 0.15036228956228279, 0.1500927410570972, 0.14965679426509212, 0.14900887260235937, 0.14885464619441205, 0.14828455627682696, 0.1474684160875987, 0.1481493124760777, 0.14913317478564847, 0.14721806200153856, 0.14780875932739443, 0.14916685219988765, 0.1490322630807578]\n",
      "vaild_train:  [0.18155935077139965, 0.16249542239193732, 0.15801524098675984, 0.15568859731921783, 0.15553964058367106, 0.15485928723445305, 0.15498607620023763, 0.15245926981935135, 0.1543110369776304, 0.15292442260453334, 0.1531637143343687, 0.1532339735959585, 0.15330183620636278, 0.15225366717920855, 0.1523370732768224, 0.15595384142719781, 0.15625585816227472]\n",
      "Epoch [18/100], Train Loss: 0.1480, Valid Loss: 0.1548\n",
      "loss_train:  [0.173994489822043, 0.15933115083769142, 0.15416382411158228, 0.15139235685388727, 0.15036228956228279, 0.1500927410570972, 0.14965679426509212, 0.14900887260235937, 0.14885464619441205, 0.14828455627682696, 0.1474684160875987, 0.1481493124760777, 0.14913317478564847, 0.14721806200153856, 0.14780875932739443, 0.14916685219988765, 0.1490322630807578, 0.14796133181416846]\n",
      "vaild_train:  [0.18155935077139965, 0.16249542239193732, 0.15801524098675984, 0.15568859731921783, 0.15553964058367106, 0.15485928723445305, 0.15498607620023763, 0.15245926981935135, 0.1543110369776304, 0.15292442260453334, 0.1531637143343687, 0.1532339735959585, 0.15330183620636278, 0.15225366717920855, 0.1523370732768224, 0.15595384142719781, 0.15625585816227472, 0.15483496538721597]\n",
      "Epoch [19/100], Train Loss: 0.1485, Valid Loss: 0.1542\n",
      "Early stopped at epoch 18, train loss stop improving\n",
      "Training finished\n",
      "452.49010848999023\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "es_cnt = 0\n",
    "es_thres = 5\n",
    "prev_train_loss = float('inf')\n",
    "loss_train = []\n",
    "loss_vaild = []\n",
    "num_epochs = 100 # 总训练轮数\n",
    "#num_batch_train = 0\n",
    "for epoch in range(num_epochs):\n",
    "  #train_bar = tqdm(train_loader)\n",
    "  train_loss = 0.0\n",
    "  \n",
    "  for i , (batch) in enumerate(train_loader):\n",
    "\n",
    "    # 数据转到device\n",
    "    train_batch = batch[0].to(device)\n",
    "    \n",
    "    # 训练步骤  \n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(train_batch)\n",
    "    loss = criterion(outputs, train_batch)\n",
    "    loss.backward() \n",
    "    optimizer.step()\n",
    "    \n",
    "    train_loss += loss.item()\n",
    "    #num_batch_train +=1\n",
    "  #train_loss除以所有bacth个数\n",
    "  train_loss = train_loss/(np.ceil(train.size(0)/batch_size1))\n",
    "  loss_train.append(train_loss)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "  # 验证\n",
    "  valid_loss = 0.0\n",
    "  #num_batch_vaild = 0\n",
    "  with torch.no_grad():\n",
    "    for i , (batch) in enumerate(vaild_loader):\n",
    "    #for batch in vaild_loader:\n",
    "    \n",
    "      val_batch = batch[0].to(device)\n",
    "      \n",
    "      outputs = model(val_batch)\n",
    "      loss = criterion(outputs, val_batch)\n",
    "      valid_loss += loss.item()\n",
    "      #num_batch_vaild += 1\n",
    "    valid_loss = valid_loss/(np.ceil(vaild.size(0)/batch_size1))\n",
    "    loss_vaild.append(valid_loss)\n",
    "    print(\"Epoch [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}\".format(epoch+1, num_epochs, train_loss, valid_loss))\n",
    "\n",
    "    \n",
    "    # Early stopping\n",
    "    if train_loss - prev_train_loss >= 0:\n",
    "        es_cnt += 1\n",
    "    else:\n",
    "        #es_cnt = 0\n",
    "        pass\n",
    "\n",
    "    if es_cnt >= es_thres:\n",
    "        print(f\"Early stopped at epoch {epoch}, train loss stop improving\")\n",
    "        break  \n",
    "\n",
    "    prev_train_loss = train_loss\n",
    "  print('loss_train: ', loss_train)\n",
    "  print('vaild_train: ',loss_vaild)          \n",
    "print(\"Training finished\")\n",
    "current_time = time.time()\n",
    "time_sum = current_time-start_time\n",
    "print(time_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train = pd.DataFrame(loss_train)\n",
    "loss_vaild = pd.DataFrame(loss_vaild)\n",
    "\n",
    "loss = pd.concat([loss_train,loss_vaild],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.columns = ['train_loss','vaild_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), r'.\\model_2dsyn11.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([33142, 2304])\n"
     ]
    }
   ],
   "source": [
    "model = AutoEncoder(w1*w2,w1*w2).to(device)\n",
    "data = data.to(device)\n",
    "model.load_state_dict(torch.load(r'.\\model_2dsyn11.pth'))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(data)\n",
    "    #loss = criterion(output, data)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output.cpu()\n",
    "output = output.numpy()\n",
    "output = pd.DataFrame(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.to_csv(r'loss_2dsyn11.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv(r'./output_2dsyn11.csv',index=None,header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss_0 = []\n",
    "#start_time = time.time()\n",
    "#es_cnt = 0\n",
    "#es_thres = 5\n",
    "#prev_train_loss = float('inf')\n",
    "#loss_train = []\n",
    "#loss_vaild = []\n",
    "#num_epochs = 100 # 总训练轮数\n",
    "##num_batch_train = 0\n",
    "#for epoch in range(num_epochs):\n",
    "#  #train_bar = tqdm(train_loader)\n",
    "#  train_loss = 0.0\n",
    "#  \n",
    "#  for i , (batch) in enumerate(train_loader):\n",
    "#\n",
    "#    # 数据转到device\n",
    "#    train_batch = batch[0].to(device)\n",
    "#    \n",
    "#    # 训练步骤  \n",
    "#    optimizer.zero_grad()\n",
    "#    outputs = model(train_batch)\n",
    "#    loss = criterion(outputs, train_batch)\n",
    "#    loss.backward() \n",
    "#    optimizer.step()\n",
    "#\n",
    "#    loss_0.append(loss.item())\n",
    "#\n",
    "#    train_loss += loss.item()\n",
    "#    #num_batch_train +=1\n",
    "#  #train_loss除以所有bacth个数\n",
    "#  train_loss = train_loss/(np.ceil(train.size(0)/batch_size1))\n",
    "#  loss_train.append(train_loss)\n",
    "#    \n",
    "#\n",
    "#\n",
    "#\n",
    "#  # 验证\n",
    "#  valid_loss = 0.0\n",
    "#  #num_batch_vaild = 0\n",
    "#  with torch.no_grad():\n",
    "#    for i , (batch) in enumerate(vaild_loader):\n",
    "#    #for batch in vaild_loader:\n",
    "#    \n",
    "#      val_batch = batch[0].to(device)\n",
    "#      \n",
    "#      outputs = model(val_batch)\n",
    "#      loss = criterion(outputs, val_batch)\n",
    "#      valid_loss += loss.item()\n",
    "#      #num_batch_vaild += 1\n",
    "#    valid_loss = valid_loss/(np.ceil(vaild.size(0)/batch_size1))\n",
    "#    loss_vaild.append(valid_loss)\n",
    "#    print(\"Epoch [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}\".format(epoch+1, num_epochs, train_loss, valid_loss))\n",
    "#\n",
    "#    \n",
    "#    # Early stopping\n",
    "#    if train_loss - prev_train_loss >= 0:\n",
    "#        es_cnt += 1\n",
    "#    else:\n",
    "#        #es_cnt = 0\n",
    "#        pass\n",
    "#\n",
    "#    if es_cnt >= es_thres:\n",
    "#        print(f\"Early stopped at epoch {epoch}, train loss stop improving\")\n",
    "#        break  \n",
    "#    \n",
    "#\n",
    "#\n",
    "##   if epoch == 10:\n",
    "##     torch.save(model.state_dict(), r'.\\model_epochs10.pth')\n",
    "##   elif epoch == 20:\n",
    "##     torch.save(model.state_dict(), r'.\\model_epochs20.pth')\n",
    "##   elif epoch == 30:\n",
    "##     torch.save(model.state_dict(), r'.\\model_epochs30.pth')\n",
    "##   elif epoch == 40:\n",
    "##     torch.save(model.state_dict(), r'.\\model_epochs40.pth')\n",
    "##   elif epoch == 50:\n",
    "##     torch.save(model.state_dict(), r'.\\model_epochs50.pth')\n",
    "##   elif epoch == 60:\n",
    "##     torch.save(model.state_dict(), r'.\\model_epochs60.pth')\n",
    "##   elif epoch == 70:\n",
    "##     torch.save(model.state_dict(), r'.\\model_epochs70.pth')\n",
    "##   elif epoch == 80:\n",
    "##     torch.save(model.state_dict(), r'.\\model_epochs80.pth')\n",
    "##   elif epoch == 90:\n",
    "##     torch.save(model.state_dict(), r'.\\model_epochs90.pth')\n",
    "##   elif epoch == 100:\n",
    "##     torch.save(model.state_dict(), r'.\\model_epochs100.pth')\n",
    "##   else:\n",
    "##       pass\n",
    "#\n",
    "#    prev_train_loss = train_loss\n",
    "#  print('loss_train: ', loss_train)\n",
    "#  print('vaild_train: ',loss_vaild)          \n",
    "#print(\"Training finished\")\n",
    "#current_time = time.time()\n",
    "#time_sum = current_time-start_time\n",
    "#print(time_sum)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
